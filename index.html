<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <!-- <meta name="description" content="DESCRIPTION META TAG"> -->
  <!-- <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/> -->
  <!-- <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/> -->
  <!-- <meta property="og:url" content="URL OF THE WEBSITE"/> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/images/your_banner_image.png" /> -->
  <!-- <meta property="og:image:width" content="1200"/> -->
  <!-- <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Diffusion Models, Unified Generation and Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Jodi: Unification of Visual Generation and Understanding via Joint Modeling</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Jodi: Unification of Visual Generation and Understanding via Joint Modeling</h1>
          
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://xyfjason.github.io/homepage" target="_blank">Yifeng Xu</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://lynnho.github.io/" target="_blank">Zhenliang He</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4AKCKKEAAAAJ" target="_blank">Meina Kan</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ" target="_blank">Shiguang Shan</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vVx2v20AAAAJ" target="_blank">Xilin Chen</a><sup>1,2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>State Key Lab of AI Safety, Institute of Computing Technology, CAS, China<br>
              <sup>2</sup>University of Chinese Academy of Sciences, China
            </span>
            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.19084" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/VIPL-GENUN/Jodi" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- HuggingFace link -->
              <span class="link-block">
                <a href="https://huggingface.co/VIPL-GENUN/Jodi" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Model</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Banner -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <img src="static/images/banner.jpg" alt="banner"/>
            <p>Our Jodi framework is capable of performing (a) joint generation, (b) controllable generation, and (c) image perception in a unified diffusion model.</p>
          </div>
      </div>
    </div>
  <!-- End banner -->
  
</div>

</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at <a href="https://github.com/VIPL-GENUN/Jodi">https://github.com/VIPL-GENUN/Jodi</a>.
          </p>
          <br/><img src="static/images/overview.png" alt="overview"/>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">

          <h2 class="title is-3">Results</h2>

          <h2 class="title is-4" style="margin-bottom: -10px;"><strong>Joint Generation</strong>: p(<span style="color:#3e8494;">x,y1,y2,...</span>)</h2>
          <div id="results-carousel" class="carousel results-carousel" style="margin-bottom: 50px;">
            <div class="item">
              <img src="static/images/joint-1.jpg" alt="joint-1"/>
            </div>
            <div class="item">
              <img src="static/images/joint-2.jpg" alt="joint-2"/>
            </div>
            <div class="item">
              <img src="static/images/joint-3.jpg" alt="joint-3"/>
            </div>
            <div class="item">
              <img src="static/images/joint-4.jpg" alt="joint-4"/>
            </div>
          </div>

          <h2 class="title is-4" style="margin-bottom: -10px;"><strong>Controllable Generation</strong>: p(<span style="color:#3e8494;">x</span>|<span style="color:#b47700">y1,y2,...</span>)</h2>
          <div id="results-carousel" class="carousel results-carousel" style="margin-bottom: 50px;">
            <div class="item">
              <img src="static/images/control-depth.jpg" alt="control-depth"/>
            </div>
            <div class="item">
              <img src="static/images/control-normal.jpg" alt="control-normal"/>
            </div>
            <div class="item">
              <img src="static/images/control-albedo.jpg" alt="control-albedo"/>
            </div>
            <div class="item">
              <img src="static/images/control-edge.jpg" alt="control-edge"/>
            </div>
            <div class="item">
              <img src="static/images/control-lineart.jpg" alt="control-lineart"/>
            </div>
            <div class="item">
              <img src="static/images/control-seg.jpg" alt="control-seg"/>
            </div>
          </div>

          <h2 class="title is-4" style="margin-bottom: -10px;"><strong>Image Perception</strong>: p(<span style="color:#3e8494;">y1,y2,...</span>|<span style="color:#b47700">x</span>)</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/percept-1.jpg" alt="percept-1"/>
            </div>
            <div class="item">
              <img src="static/images/percept-2.jpg" alt="percept-2"/>
            </div>
            <div class="item">
              <img src="static/images/percept-3.jpg" alt="percept-3"/>
            </div>
            <div class="item">
              <img src="static/images/percept-4.jpg" alt="percept-4"/>
            </div>
            <div class="item">
              <img src="static/images/percept-5.jpg" alt="percept-5"/>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Dataset -->
<section class="hero is-light">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Joint-1.6M Dataset</h2>
        <div class="content has-text-justified">
          We collect images with high quality and diversity from several publicly available sources, including <a href="https://huggingface.co/datasets/Yuanshi/Subjects200K_collection3">Subjects200K</a>, <a href="https://huggingface.co/datasets/zhang0jhon/Aesthetic-4K">Aesthetic-4K</a>, and <a href="https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf">Pexels</a>. All of these images have resolutions over 1024&times;1024, which is advantageous for training a high-resolution generative model. As these datasets lack labels, we use state-of-the-art predictors to automatically annotate the data with labels corresponding to 7 specific domains. Specifically, we employ <a href="https://github.com/carolineec/informative-drawings">Informative Drawings</a> to generate line arts, <a href="https://github.com/hellozhuo/pidinet">PiDiNet</a> to extract edge maps, <a href="https://github.com/DepthAnything/Depth-Anything-V2">Depth Anything V2</a> and <a href="https://github.com/EnVision-Research/Lotus">Lotus</a> to estimate depth maps, <a href="https://github.com/EnVision-Research/Lotus">Lotus</a> to estimate normal maps, <a href="https://github.com/zheng95z/rgbx">RGB2X</a> to estimate albedos, <a href="https://github.com/SHI-Labs/OneFormer">Oneformer</a> to predict segmentation colormaps, and <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">Openpose</a> to predict human skeletons. In this manner, we construct a dataset containing 200,000 images with corresponding 7&times;200,000 predicted labels. Furthermore, we use <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b">BLIP2-OPT-2.7b</a> and <a href="https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct">Qwen2-VL-7b-Instruct</a> to generate captions. The former tends to provide a concise description of the main subject in the image, while the latter tends to give a long paragraph that describes the subject, background, and the overall atmosphere in detail.
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- End dataset -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">BibTex</h2>
        <pre><code>@article{xu2025jodi,
  title={Jodi: Unification of Visual Generation and Understanding via Joint Modeling},
  author={Xu, Yifeng and He, Zhenliang and Kan, Meina and Shan, Shiguang and Chen, Xilin},
  journal={arXiv preprint arXiv:2505.19084},
  year={2025}
}</code></pre>
      </div>
    </div>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.<br/>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- Default Statcounter code for Jodi Project Page
https://vipl-genun.github.io/Project-Jodi/ -->
<script type="text/javascript">
var sc_project=13139078; 
var sc_invisible=0; 
var sc_security="a61ec63b"; 
var scJsHost = "https://";
document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/13139078/0/a61ec63b/0/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>

<!-- End of Statcounter Code -->

</body>
</html>
